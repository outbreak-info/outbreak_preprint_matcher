{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outbreak resource litcovid and preprint matcher\n",
    "\n",
    "This code pings the outbreak.info api to pull an updated list of ids, compares the ids with files containing previously run ids and identifies the newly updated ids. For the newly updated ids, it pings the api to pull the relevant metadata so that a similarity test can be run for the new ids.\n",
    "\n",
    "**Requirements**\n",
    "This code was written in python 3.6 and uses the following libraries:\n",
    "* requests\n",
    "* pickle\n",
    "* json\n",
    "* pandas\n",
    "* nltk\n",
    "* string\n",
    "* datetime\n",
    "\n",
    "**Limitations**\n",
    "This code does not account for publications hosted in Zenodo, Dataverse, Figshare, or any other general repository, as the relationship between publications hosted on those sites and litcovid publications cannot be automatically determined.  This code is only for linking preprints in biorxiv and medrxiv to litcovid. Note that it currently does not accommodate preprint rxivs outside of biorxiv and medrxiv as the parsers for those preprints have yet to be written.\n",
    "\n",
    "**Assumptions**\n",
    "In order to minimize manual review, the thresholds have been set pretty high so precision is expected to be high, but sensitivity is expected to be low. An initial run was already performed and all the relevant data was already saved.  This data is included in the repo as detailed below\n",
    "\n",
    "**File structure**\n",
    "Previous results are 'cached' (ie-saved and updated), so that recalculations are not required, and time isn't wasted re-running\n",
    "Files may be named by type of meta compared (either 'text' or 'auth' (author)), and source (either 'litcovid' or 'preprint')\n",
    "\n",
    "**file paths**:\n",
    "* 'results/archives/' - stores precomputed files from previous runs and lists of identifiers in previous runs\n",
    "* 'temp/' - temporarily stores the type-specific successful matches in a run\n",
    "* 'to review/' - stores the results of the matching that require manual review\n",
    "* 'update dumps/' - stores the dataframe of updates to make based on sorted matches in this run\n",
    "\n",
    "**Pre-existing files**\n",
    "* 'results/archives/all_`source`_ids.txt' - a pickled list of identifiers that has already been run (where `source` is either litcovid or preprints\n",
    "* 'results/archives/`compare_type`_`source`_set.txt' - a pickled pandas dataframe containing preprocessed text for comparison. The `source` is again either litcovid or preprints, while the `compare_type` is either auth (author), or text (title and abstract)\n",
    "* 'temp/`compare_type`_above_threshold.txt - a tab-delimited text file containing all matches based on the `compare_type` (either auth or text) where the similarity was found to be above the minimum threshold. These files are merged to identify match candidates\n",
    "* 'results/to review/low_scores.txt' - a tab-delimited pandas dump for matches where the sum score was below the threshold for acceptance\n",
    "* 'results/to review/manual_check.txt' - a tab-delimited pandas dump for matches where a litcovid item matched with more than one preprint or vice versa\n",
    "* 'results/archives/clean_results.txt' - a tab-delimited pandas dump for matches which do not need further screening. This file is processed for creating the update dump\n",
    "* 'results/update dumps/update_file.txt' - a tab-delimited pandas dump for matches which do not need further screening and have been formatted with the appropriate fields for importing into outbreak.info resource metadata\n",
    "\n",
    "Note that the script has been broken up into parts for ease of re-running and troubleshooting. It also uses topicCategories generated by the topic classifier, and date information to limit the number of comparisons to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "from datetime import datetime,timedelta\n",
    "import pathlib\n",
    "\n",
    "\n",
    "#### Set paths\n",
    "#scriptpath = pathlib.Path(__file__).parent.absolute()\n",
    "#try:\n",
    "#    generalpath = pathlib.Path(__file__).parents[1].absolute()\n",
    "#except:\n",
    "#    generalpath = pathlib.Path(__file__).resolve().parents[1].absolute()\n",
    "scriptpath = ''\n",
    "RESULTSPATH = os.path.join(scriptpath,'results/')\n",
    "ARCHIVEPATH = os.path.join(RESULTSPATH,'archives/')\n",
    "TEMPPATH = os.path.join(RESULTSPATH,'temp/')\n",
    "OUTPUTPATH = os.path.join(RESULTSPATH,'update dumps/')\n",
    "REVIEWPATH = os.path.join(RESULTSPATH,'to review/')\n",
    "TOPICPATH = ''\n",
    "#TOPICPATH = os.path.join(generalpath,'topic_classifier/results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to fetch all ids and metadata from a source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the size of the source (to make it easy to figure out when to stop scrolling)\n",
    "def fetch_src_size(source):\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=curatedBy.name:\"+source+\"&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_source_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/query?q=curatedBy.name:\"+source+\"&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/query?q=curatedBy.name:\"+source+\"&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "#### Pull ids from the major publication sources (litcovid, medrxiv,biorxiv)\n",
    "def get_pub_ids():\n",
    "    update_date = datetime.now()\n",
    "    biorxiv_ids = get_source_ids(\"bioRxiv\")\n",
    "    medrxiv_ids = get_source_ids(\"medRxiv\")\n",
    "    litcovid_idlist = get_source_ids(\"litcovid\")\n",
    "    preprint_idlist = list(set(medrxiv_ids).union(set(biorxiv_ids)))\n",
    "    preprint_dict={datetime.strftime(update_date,'%Y-%m-%d'):preprint_idlist}\n",
    "    litcovid_dict={datetime.strftime(update_date,'%Y-%m-%d'):litcovid_idlist}\n",
    "    return(preprint_dict,litcovid_dict)\n",
    "\n",
    "\n",
    "def get_date(datedict):\n",
    "    for eachdate in list(datedict.keys()):\n",
    "        dict_date = datetime.strptime(eachdate,'%Y-%m-%d')\n",
    "    return(dict_date)\n",
    "\n",
    "#### Load the previously saved id lists, and compare the two to identify only the new ids\n",
    "def remove_old_ids(preprint_dict,litcovid_dict,ARCHIVEPATH,TEMPPATH):\n",
    "    preprint_run = pickle.load(open(os.path.join(ARCHIVEPATH,\"all_preprint_dict.txt\"), \"rb\"))\n",
    "    litcovid_run = pickle.load(open(os.path.join(ARCHIVEPATH,\"all_litcovid_dict.txt\"), \"rb\"))\n",
    "    old_preprint_date = get_date(preprint_run)\n",
    "    old_litcovid_date = get_date(litcovid_run)\n",
    "    all_preprint_date = get_date(preprint_dict)\n",
    "    all_litcovid_date = get_date(litcovid_dict)\n",
    "    old_pre_str_date = datetime.strftime(old_preprint_date,'%Y-%m-%d')\n",
    "    old_lit_str_date = datetime.strftime(old_litcovid_date,'%Y-%m-%d')\n",
    "    all_pre_str_date = datetime.strftime(all_preprint_date,'%Y-%m-%d')\n",
    "    all_lit_str_date = datetime.strftime(all_litcovid_date,'%Y-%m-%d')\n",
    "    if (all_preprint_date-old_preprint_date)> timedelta(days=1):\n",
    "        new_preprint_ids = [x for x in preprint_dict[all_pre_str_date] if x not in preprint_run[old_pre_str_date]]\n",
    "        new_preprint_dict = {all_pre_str_date:new_preprint_ids}\n",
    "        with open(os.path.join(TEMPPATH,\"new_preprint_dict.txt\"),\"wb\") as dumpfile:\n",
    "            pickle.dump(new_preprint_dict,dumpfile)\n",
    "        with open(os.path.join(ARCHIVEPATH,\"all_preprint_dict.txt\"),\"wb\") as dumpfile:\n",
    "            pickle.dump(preprint_dict,dumpfile)       \n",
    "    if (all_litcovid_date-old_litcovid_date)> timedelta(days=1):\n",
    "        new_litcovid_ids = [x for x in litcovid_dict[all_lit_str_date] if x not in litcovid_run[old_lit_str_date]]\n",
    "        new_litcovid_dict = {all_lit_str_date:new_litcovid_ids}\n",
    "        with open(os.path.join(TEMPPATH,\"new_litcovid_dict.txt\"),\"wb\") as dumpfile:\n",
    "            pickle.dump(new_litcovid_dict,dumpfile)\n",
    "        with open(os.path.join(ARCHIVEPATH,\"all_litcovid_dict.txt\"),\"wb\") as dumpfile:\n",
    "            pickle.dump(litcovid_dict,dumpfile)  \n",
    "            \n",
    "            \n",
    "def check_id_update_status(TEMPPATH):\n",
    "    today = datetime.now()\n",
    "    preprint_run = pickle.load(open(os.path.join(TEMPPATH,\"new_preprint_dict.txt\"), \"rb\"))\n",
    "    old_preprint_date = get_date(preprint_run)\n",
    "    litcovid_run = pickle.load(open(os.path.join(TEMPPATH,\"new_litcovid_dict.txt\"), \"rb\"))\n",
    "    old_litcovid_date = get_date(litcovid_run)\n",
    "    run_dict = {'preprint_updated':False,'litcovid_updated':False}\n",
    "    if (today-old_preprint_date) < timedelta(days = 1):\n",
    "        run_dict['preprint_updated']=True\n",
    "    if (today-old_litcovid_date) < timedelta(days = 1):\n",
    "        run_dict['litcovid_updated']=True\n",
    "    return(run_dict)\n",
    "\n",
    "\n",
    "def run_id_update(ARCHIVEPATH,TEMPPATH):\n",
    "    run_dict = check_id_update_status(TEMPPATH)\n",
    "    if False in list(run_dict.values()):\n",
    "        all_preprint_dict,all_litcovid_dict = get_pub_ids()\n",
    "        remove_old_ids(all_preprint_dict,all_litcovid_dict,ARCHIVEPATH,TEMPPATH)\n",
    "        run_dict = check_id_update_status(ARCHIVEPATH)\n",
    "        return(run_dict)\n",
    "    else:\n",
    "        return(run_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_ids(TEMPPATH):\n",
    "    new_preprint_dict = pickle.load(open(os.path.join(TEMPPATH,\"new_preprint_dict.txt\"), \"rb\"))\n",
    "    preprintdatekey = list(new_preprint_dict.keys())[0]\n",
    "    new_preprint_ids = new_preprint_dict[preprintdatekey]\n",
    "    new_litcovid_dict = pickle.load(open(os.path.join(TEMPPATH,\"new_litcovid_dict.txt\"), \"rb\"))\n",
    "    litcoviddatekey = list(new_litcovid_dict.keys())[0]\n",
    "    new_litcovid_ids = new_litcovid_dict[litcoviddatekey]\n",
    "    return(new_preprint_ids,new_litcovid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the metadata for each list\n",
    "#### Note, I've tried batches of 1000, and the post request has failed, so this uses a batch size that's less likely to fail\n",
    "def batch_fetch_meta(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    textdf = pd.DataFrame(columns = ['_id','abstract','name','date'])\n",
    "    authdf = pd.DataFrame(columns = ['_id','author','date'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        ## Get the text-based metadata (abstract, title) and save it\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract,date'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = pd.read_json(r.text)\n",
    "            cleanresult = rawresult[['_id','name','abstract','date']].loc[rawresult['_score']==1].copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            textdf = pd.concat((textdf,cleanresult),ignore_index=True)\n",
    "        ## Get the author metadata and save it    \n",
    "        a = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'author,date'})\n",
    "        if a.status_code == 200:\n",
    "            rawresult = pd.read_json(a.text)\n",
    "            cleanresult = rawresult[['_id','author','date']].loc[rawresult['_score']==1].copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            authdf = pd.concat((authdf,cleanresult),ignore_index=True)\n",
    "        i=i+1\n",
    "    return(textdf,authdf)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for cleaning up metadata for new entries prior to running comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce camelcase differences by lower casing everything, deal with punctuation oddities, remove stopwords and tokenize\n",
    "def text2word_tokens(section_text):\n",
    "    sample_text = section_text.lower().translate(str.maketrans('','',string.punctuation))\n",
    "    sample_set = [x for x in nltk.tokenize.word_tokenize(sample_text) if x not in stopwords]\n",
    "    return(sample_set)\n",
    "\n",
    "## Pull the ids from a dataframe\n",
    "def get_ids_from_df(rawdf_set):\n",
    "    rawdf_ids = rawdf_set['_id'].unique().tolist()\n",
    "    return(rawdf_ids)\n",
    "\n",
    "## merge title and abstract and create bag of words, remove entries missing abstract (can't be compared)\n",
    "def remove_text_na(rawdf):\n",
    "    rawdf['text'] = rawdf['name'].str.cat(rawdf['abstract'], sep=\" | \")\n",
    "    rawdf_set = rawdf.loc[~rawdf['abstract'].isna() & ~rawdf['text'].isna()].copy()\n",
    "    rawdf_set['words'] = rawdf_set.apply(lambda x: text2word_tokens(x['text']), axis=1)\n",
    "    return(rawdf_set)\n",
    "\n",
    "## create bag of words from author and remove entries missing authors (can't be compared)\n",
    "def remove_auth_na(rawdf,textset_ids):\n",
    "    rawdf_set = rawdf.loc[~rawdf['author'].isna() & rawdf['_id'].isin(textset_ids)].copy()\n",
    "    rawdf_set['author'] = rawdf_set['author'].astype(str)\n",
    "    rawdf_set['words'] = rawdf_set.apply(lambda x: text2word_tokens(x['author']), axis=1)\n",
    "    return(rawdf_set)\n",
    "\n",
    "## run the cleaning functions above on a given text dataframe, author dataframe, and source (preprint or litcovid)\n",
    "def clean_source_data(textdf,authdf,source):\n",
    "    textdf_set = remove_text_na(textdf)\n",
    "    textdf_ids = get_ids_from_df(textdf_set)\n",
    "    authdf_set = remove_auth_na(authdf,textdf_ids)\n",
    "    authdf_ids = get_ids_from_df(authdf_set)\n",
    "    return(textdf_set,authdf_set)\n",
    "\n",
    "## Remove previous successful matches from old metadata prior to running comparisons\n",
    "def remove_matched_values(source,dftype,ARCHIVEPATH):\n",
    "    clean_matches = read_csv(os.path.join(ARCHIVEPATH,'clean_results.txt'),delimiter='\\t',header=0,index_col=0)\n",
    "    matched_ids = clean_matches[source].unique().tolist()\n",
    "    filename = dftype+\"_\"+source+\"_set.txt\"\n",
    "    with open(os.path.join(ARCHIVEPATH,filename), \"rb\") as openfile:\n",
    "        old_source = pickle.load(openfile)\n",
    "    clean_source = old_source.loc[~old_source['_id'].isin(matched_ids)]\n",
    "    return(clean_source)\n",
    "\n",
    "\n",
    "## Blank out the previous temp files \n",
    "def blank_temps(TEMPPATH):\n",
    "    tmpfiles = ['auth_above_threshold.txt','text_above_threshold.txt']\n",
    "    for eachfile in tmpfiles:\n",
    "        with open(os.path.join(TEMPPATH,eachfile),'w') as outwrite:\n",
    "            outwrite.write('\\tlitcovid\\tpreprint\\tj_sim\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_meta(ARCHIVEPATH,TEMPPATH):\n",
    "    run_dict = run_id_update(ARCHIVEPATH,TEMPPATH)\n",
    "    if False not in list(run_dict.values()):\n",
    "        new_preprint_ids,new_litcovid_ids = load_new_ids(TEMPPATH)\n",
    "\n",
    "        clean_rxiv_text = pickle.load(open(os.path.join(TEMPPATH,\"clean_rxiv_text.txt\"), \"rb\"))\n",
    "        clean_rxiv_ids = get_ids_from_df(clean_rxiv_text)\n",
    "        if (len(set(new_preprint_ids).intersection(set(clean_rxiv_ids)))/len(new_preprint_ids))<0.66:\n",
    "            new_preprint_textdf,new_preprint_authdf = batch_fetch_meta(new_preprint_ids)\n",
    "            clean_rxiv_text,clean_rxiv_auth = clean_source_data(new_preprint_textdf,new_preprint_authdf,'preprint')\n",
    "            with open(os.path.join(TEMPPATH,\"clean_rxiv_text.txt\"), \"wb\") as dmpfile:\n",
    "                pickle.dump(clean_rxiv_text, dmpfile)\n",
    "            with open(os.path.join(TEMPPATH,\"clean_rxiv_auth.txt\"), \"wb\") as dmpfile:\n",
    "                pickle.dump(clean_rxiv_auth, dmpfile)\n",
    "\n",
    "        clean_litcovid_text = pickle.load(open(os.path.join(TEMPPATH,\"clean_lit_text.txt\"), \"rb\"))\n",
    "        clean_litcovid_ids = get_ids_from_df(clean_litcovid_text)\n",
    "        if (len(set(new_litcovid_ids).intersection(set(clean_litcovid_ids)))/len(new_litcovid_ids))<0.66:\n",
    "            new_litcovid_textdf,new_litcovid_authdf = batch_fetch_meta(new_litcovid_ids)\n",
    "            clean_lit_text,clean_lit_auth = clean_source_data(new_litcovid_textdf,new_litcovid_authdf,'litcovid')\n",
    "            with open(os.path.join(TEMPPATH,\"clean_lit_text.txt\"), \"wb\") as dmpfile:\n",
    "                pickle.dump(clean_lit_text, dmpfile)\n",
    "            with open(os.path.join(TEMPPATH,\"clean_lit_auth.txt\"), \"wb\") as dmpfile:\n",
    "                pickle.dump(clean_lit_auth, dmpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for updating the save files\n",
    "\n",
    "## Function to update the bag of words dataframes\n",
    "def update_precompute(clean_df_set,ARCHIVEPATH):\n",
    "    if 'pmid' in clean_df_set['_id'].iloc[0]:\n",
    "        df_source = \"litcovid\"\n",
    "    else:\n",
    "        df_source = \"preprint\"\n",
    "    if 'author' in list(clean_df_set.columns):\n",
    "        df_type = 'auth'\n",
    "    else:\n",
    "        df_type = 'text'\n",
    "    filename = df_type+\"_\"+df_source+\"_set.txt\"\n",
    "    with open(os.path.join(ARCHIVEPATH,filename), \"rb\") as tmpfile:\n",
    "        old_info = pickle.load(tmpfile)\n",
    "    updated_info = pd.concat((old_info,clean_df_set),ignore_index=True)\n",
    "    updated_info.drop_duplicates(keep='last',inplace=True)\n",
    "    with open(os.path.join(ARCHIVEPATH,filename), \"wb\") as dmpfile:\n",
    "        pickle.dump(updated_info, dmpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## load the metadata for comparison\n",
    "def load_new_meta(TEMPPATH):\n",
    "    clean_rxiv_text = pickle.load(open(os.path.join(TEMPPATH,\"clean_rxiv_text.txt\"),\"rb\"))\n",
    "    clean_rxiv_auth = pickle.load(open(os.path.join(TEMPPATH,\"clean_rxiv_auth.txt\"),\"rb\"))\n",
    "    clean_lit_text = pickle.load(open(os.path.join(TEMPPATH,\"clean_lit_text.txt\"),\"rb\"))\n",
    "    clean_lit_auth = pickle.load(open(os.path.join(TEMPPATH,\"clean_lit_auth.txt\"),\"rb\"))\n",
    "    return(clean_rxiv_text,clean_rxiv_auth,clean_lit_text,clean_lit_auth)\n",
    "\n",
    "def load_previous_runs(ARCHIVEPATH):\n",
    "    ## Load previous run and remove successfully mapped entries\n",
    "    old_rxiv_text = remove_matched_values('preprint','text',ARCHIVEPATH)\n",
    "    old_rxiv_auth = remove_matched_values('preprint','auth',ARCHIVEPATH)\n",
    "    old_lit_text = remove_matched_values('litcovid','text',ARCHIVEPATH)\n",
    "    old_lit_auth = remove_matched_values('litcovid','auth',ARCHIVEPATH)\n",
    "    return(old_rxiv_text,old_rxiv_auth,old_lit_text,old_lit_auth)\n",
    "\n",
    "#### Check if 2/3 of the new ids are already in the old ids. If so, then there's no need to do the comparison\n",
    "#### Ideally, we'd want to see if 100% of new ids are in the old ids, but if an id was dropped due to lack of\n",
    "#### text or any other error, this may trigger an unnecessary re-run, so it's relaxed to 2/3 \n",
    "def check_b4_compare(newdf,olddf):\n",
    "    newids = set(newdf['_id'].unique().tolist())\n",
    "    oldids = set(olddf['_id'].unique().tolist())\n",
    "    incommon = oldids.intersection(newids)\n",
    "    if len(incommon)/len(newids)>0.67:\n",
    "        newdata = False ## There isn't much new data that isn't already in the archived data\n",
    "    else:\n",
    "        newdata = True ## New data available\n",
    "    return(newdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate id_lists by topicCategory  \n",
    "def generate_comparison_dfs(topicdf,litcoviddf,preprintdf,topicCategory):\n",
    "    if topicCategory==None:\n",
    "        idlist = topicdf['_id'].unique().tolist()\n",
    "        preprint_topicdf = preprintdf.loc[~preprintdf['_id'].isin(idlist)]\n",
    "        litcovid_topicdf = litcoviddf.loc[~litcoviddf['_id'].isin(idlist)]         \n",
    "    else:\n",
    "        idlist = topicdf['_id'].loc[topicdf['topicCategory']==topicCategory].unique().tolist()\n",
    "        alltopicids = topicdf['_id'].unique().tolist()\n",
    "        preprint_topicdf = preprintdf.loc[((preprintdf['_id'].isin(idlist))|\n",
    "                                          (~preprintdf['_id'].isin(alltopicids)))]\n",
    "        litcovid_topicdf = litcoviddf.loc[((litcoviddf['_id'].isin(idlist))|\n",
    "                                          (~litcoviddf['_id'].isin(alltopicids)))]       \n",
    "    return(preprint_topicdf,litcovid_topicdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to perform comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jsim(sample_set1,sample_set2,thresholds,set_type):\n",
    "    j_dist = nltk.jaccard_distance(set(sample_set1),set(sample_set2))\n",
    "    j_sim = 1-j_dist\n",
    "    if j_sim > thresholds[set_type]:\n",
    "        return(j_sim)\n",
    "    else:\n",
    "        return(-1)\n",
    "    \n",
    "\n",
    "## The comparison function re-written with pandas itterrows AND lambda apply in hopes of speeding it up even more\n",
    "def run_comparison(preprint_set,litcovid_set,set_type,thresholds,TEMPPATH):\n",
    "    matches = pd.DataFrame(columns=['litcovid','preprint','j_sim'])\n",
    "    filename = set_type+\"_above_threshold.txt\"\n",
    "    for index, row in litcovid_set.iterrows():\n",
    "        litcovidwords = set(row['words'])\n",
    "        preprint_subset = preprint_set.loc[preprint_set['date']<=row['date']].copy()\n",
    "        if len(preprint_subset)>0:\n",
    "            preprint_subset.rename(columns={'_id':'preprint'},inplace=True)\n",
    "            preprint_subset['j_sim'] = preprint_subset.apply(lambda x: calculate_jsim(litcovidwords,set(x['words']),thresholds,set_type),axis=1)\n",
    "            preprint_subset['litcovid']=row['_id']\n",
    "            clean = preprint_subset[['litcovid','preprint','j_sim']].loc[preprint_subset['j_sim']!=-1].copy()\n",
    "            if len(clean)>0:\n",
    "                matches = pd.concat((matches,clean),ignore_index=True)\n",
    "    matches.to_csv(os.path.join(TEMPPATH,filename),mode=\"a\",sep='\\t',header=False) \n",
    "    \n",
    "    \n",
    "## Merge the author and text matches that meet threshold, calculate sum score, and sort results\n",
    "def sort_matches(new_text_matches,new_auth_matches,threshold):\n",
    "    new_text_matches.rename(columns={'j_sim':'j_sim_text'},inplace=True)\n",
    "    new_auth_matches.rename(columns={'j_sim':'j_sim_author'},inplace=True)\n",
    "    preprint_matches = new_text_matches.merge(new_auth_matches,on=['litcovid','preprint'],how='inner')\n",
    "    preprint_matches['sum_score'] = preprint_matches['j_sim_text']+preprint_matches['j_sim_author']\n",
    "    preprint_matches['date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    ## Set duplicates aside for manual checking\n",
    "    dupcheckdf = preprint_matches.groupby('preprint').size().reset_index(name='preprint_count')\n",
    "    dup_preprints = dupcheckdf['preprint'].loc[dupcheckdf['preprint_count']>1].tolist() ## does a preprint map to more than one pmid?\n",
    "    duplitcheckdf = preprint_matches.groupby('litcovid').size().reset_index(name='litcovid_count')\n",
    "    dup_pmids = duplitcheckdf['litcovid'].loc[duplitcheckdf['litcovid_count']>1].tolist() ## does a preprint map to more than one pmid?\n",
    "        \n",
    "    duplicates = preprint_matches.loc[(preprint_matches['litcovid'].isin(dup_pmids)) | \n",
    "                                      (preprint_matches['preprint'].isin(dup_preprints))]\n",
    "    ## Set low scores aside for manual checking\n",
    "    lowscores = preprint_matches.loc[preprint_matches['sum_score']<threshold['sum_min']]\n",
    "    ## Save the clean matches for auto updating\n",
    "    clean_matches = preprint_matches.loc[(~preprint_matches['litcovid'].isin(dup_pmids)) &\n",
    "                                         (~preprint_matches['preprint'].isin(dup_preprints)) &\n",
    "                                         (preprint_matches['sum_score']>=threshold['sum_min'])]\n",
    "\n",
    "    manual_check = pd.concat((duplicates,lowscores),ignore_index=True)\n",
    "    return(clean_matches,lowscores,manual_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for cleaning up the results and caching processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format the results for easier updating in biothings\n",
    "def convert_txt_dumps(txtdump):\n",
    "    txtdump.rename(columns={'correction.identifier':'identifier','correction.url':'url','correction.correctionType':'correctionType'}, inplace=True)\n",
    "    dictlist = []\n",
    "    for i in range(len(txtdump)):\n",
    "        tmpdict={'_id':txtdump.iloc[i]['_id'],'correction':[{'@type':'Correction',\n",
    "                                                            'identifier':txtdump.iloc[i]['identifier'],\n",
    "                                                            'correctionType':txtdump.iloc[i]['correctionType'],\n",
    "                                                            'url':txtdump.iloc[i]['url']}]}\n",
    "        dictlist.append(tmpdict)\n",
    "    return(dictlist)\n",
    "\n",
    "def generate_updates(updatedf,OUTPUTPATH):\n",
    "    priorupdates = read_csv(os.path.join(OUTPUTPATH,'update_file.tsv'),delimiter=\"\\t\",header=0,index_col=0)\n",
    "    correctionA = updatedf[['litcovid','preprint']].copy()\n",
    "    correctionA.rename(columns={'litcovid':'_id','preprint':'correction.identifier'},inplace=True)\n",
    "    correctionA['@type']='outbreak:Correction'\n",
    "    correctionA['correction.correctionType']='preprint'\n",
    "    correctionA['baseurl']='https://doi.org/10.1101/'\n",
    "    correctionA['correction.url']=correctionA['baseurl'].str.cat(correctionA['correction.identifier'])\n",
    "    correctionA.drop('baseurl',axis=1,inplace=True)\n",
    "    correctionB = updatedf[['litcovid','preprint']].copy()\n",
    "    correctionB.rename(columns={'litcovid':'correction.identifier','preprint':'_id'},inplace=True)\n",
    "    correctionB['@type']='outbreak:Correction'\n",
    "    correctionB['correction.correctionType']='peer-reviewed version'\n",
    "    correctionB['baseurl']='https://pubmed.ncbi.nlm.nih.gov/'\n",
    "    correctionB['correction.url']=correctionB['baseurl'].str.cat(correctionB['correction.identifier'])\n",
    "    correctionB.drop('baseurl',axis=1,inplace=True)\n",
    "    correctionupdate = pd.concat((priorupdates,correctionA,correctionB),ignore_index=True)\n",
    "    correctionupdate.drop_duplicates(keep='first')\n",
    "    correctionupdate.to_csv(os.path.join(OUTPUTPATH,'update_file.tsv'),sep=\"\\t\",header=True)\n",
    "    corrections_added = len(correctionupdate)\n",
    "    json_corrections = convert_txt_dumps(correctionupdate)\n",
    "    with open(os.path.join(OUTPUTPATH,'update_file.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_corrections, f)\n",
    "    return(corrections_added)\n",
    "\n",
    "def generate_split_updates(updatedf,OUTPUTPATH):\n",
    "    priorlitcovidupdates = read_csv(os.path.join(OUTPUTPATH,'litcovid_update_file.tsv'),delimiter=\"\\t\",header=0,index_col=0)\n",
    "    correctionA = updatedf[['litcovid','preprint']].copy()\n",
    "    correctionA.rename(columns={'litcovid':'_id','preprint':'correction.identifier'},inplace=True)\n",
    "    correctionA['@type']='outbreak:Correction'\n",
    "    correctionA['correction.correctionType']='preprint'\n",
    "    correctionA['baseurl']='https://doi.org/10.1101/'\n",
    "    correctionA['correction.url']=correctionA['baseurl'].str.cat(correctionA['correction.identifier'])\n",
    "    correctionA.drop('baseurl',axis=1,inplace=True)\n",
    "    correctionAupdate = pd.concat((priorlitcovidupdates,correctionA),ignore_index=True)\n",
    "    correctionAupdate.drop_duplicates(keep='first')\n",
    "    correctionAupdate.to_csv(os.path.join(OUTPUTPATH,'litcovid_update_file.tsv'),sep=\"\\t\",header=True)\n",
    "    json_correctionsA = convert_txt_dumps(correctionAupdate)\n",
    "    with open(os.path.join(OUTPUTPATH,'litcovid_update_file.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_correctionsA, f)\n",
    "\n",
    "    priorpreprintupdates = read_csv(os.path.join(OUTPUTPATH,'preprint_update_file.tsv'),delimiter=\"\\t\",header=0,index_col=0)    \n",
    "    correctionB = updatedf[['litcovid','preprint']].copy()\n",
    "    correctionB.rename(columns={'litcovid':'correction.identifier','preprint':'_id'},inplace=True)\n",
    "    correctionB['@type']='outbreak:Correction'\n",
    "    correctionB['correction.correctionType']='peer-reviewed version'\n",
    "    correctionB['baseurl']='https://pubmed.ncbi.nlm.nih.gov/'\n",
    "    correctionB['correction.url']=correctionB['baseurl'].str.cat(correctionB['correction.identifier'])\n",
    "    correctionB.drop('baseurl',axis=1,inplace=True)\n",
    "    correctionBupdate = pd.concat((priorpreprintupdates,correctionB),ignore_index=True)\n",
    "    correctionBupdate.drop_duplicates(keep='first')\n",
    "    correctionBupdate.to_csv(os.path.join(OUTPUTPATH,'preprint_update_file.tsv'),sep=\"\\t\",header=True)\n",
    "    json_correctionsB = convert_txt_dumps(correctionBupdate)\n",
    "    with open(os.path.join(OUTPUTPATH,'preprint_update_file.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_correctionsB, f)\n",
    "    corrections_added = len(correctionBupdate)+len(correctionAupdate)\n",
    "    return(corrections_added)\n",
    "\n",
    "\n",
    "## Function to update the save files for manual review or further processing (formatting for biothings)        \n",
    "def update_results(result_df,ARCHIVEPATH,REVIEWPATH):\n",
    "    update_dict = {}\n",
    "    dupcheck = result_df.groupby('litcovid').size().reset_index(name='counts')\n",
    "    dupcheck2 = result_df.groupby('preprint').size().reset_index(name='counts')\n",
    "    if len(dupcheck.loc[dupcheck['counts']>1]) or len(dupcheck2.loc[dupcheck2['counts']>1]):\n",
    "        old_manual_check = read_csv(os.path.join(REVIEWPATH,'manual_check.txt'),delimiter='\\t',header=0,index_col=0)\n",
    "        update_dict['previous matches for manual checking']=len(old_manual_check)\n",
    "        update_dict['current matches for manual checking'] =len(result_df)\n",
    "        total_manual_check = pd.concat((old_manual_check,result_df),ignore_index=True)\n",
    "        total_manual_check.drop_duplicates(subset=['litcovid','preprint'],keep='first',inplace=True)\n",
    "        total_manual_check.to_csv(os.path.join(REVIEWPATH,'manual_check.txt'),sep='\\t',header=True)\n",
    "    elif result_df['sum_score'].max() < 0.75:\n",
    "        old_low_scores = read_csv(os.path.join(REVIEWPATH,'low_scores.txt'),delimiter='\\t',header=0,index_col=0)\n",
    "        update_dict['previous matches with low scores']=len(old_low_scores)\n",
    "        update_dict['current matches with low scores'] =len(result_df)\n",
    "        old_low_scores = pd.concat((old_low_scores,result_df),ignore_index=True)\n",
    "        old_low_scores.drop_duplicates(subset=['litcovid','preprint'],keep='first',inplace=True)\n",
    "        old_low_scores.to_csv(os.path.join(REVIEWPATH,'low_scores.txt'),sep='\\t',header=True)\n",
    "    elif (len(dupcheck) == len(result_df)) and (len(dupcheck2)==len(result_df)):\n",
    "        old_clean_results = read_csv(os.path.join(ARCHIVEPATH,'clean_results.txt'),delimiter='\\t',header=0,index_col=0)\n",
    "        update_dict['previous matches for updating']=len(old_clean_results)\n",
    "        update_dict['current matches for updating'] =len(result_df)\n",
    "        old_clean_results = pd.concat((old_clean_results,result_df),ignore_index=True)\n",
    "        old_clean_results.drop_duplicates(subset=['litcovid','preprint'],keep='first',inplace=True)\n",
    "        old_clean_results.to_csv(os.path.join(ARCHIVEPATH,'clean_results.txt'),sep='\\t',header=True)\n",
    "    return(update_dict)   \n",
    "\n",
    "\n",
    "def check_comparison_run(TEMPPATH):\n",
    "    clean_rxiv_file = pathlib.Path(os.path.join(TEMPPATH,'clean_rxiv_text.txt'))\n",
    "    clean_lit_file = pathlib.Path(os.path.join(TEMPPATH,'clean_lit_text.txt'))\n",
    "    auth_matches = pathlib.Path(os.path.join(TEMPPATH,'auth_above_threshold.txt'))\n",
    "    text_matches = pathlib.Path(os.path.join(TEMPPATH,'text_above_threshold.txt'))\n",
    "    rxiv_mtime = datetime.fromtimestamp(clean_rxiv_file.stat().st_mtime)\n",
    "    lit_mtime = datetime.fromtimestamp(clean_lit_file.stat().st_mtime)\n",
    "    text_mtime = datetime.fromtimestamp(text_matches.stat().st_mtime)\n",
    "    auth_mtime = datetime.fromtimestamp(auth_matches.stat().st_mtime)\n",
    "    text_size = text_matches.stat().st_size/1024\n",
    "    auth_size = auth_matches.stat().st_size/1024\n",
    "    blank_size = 27/1024\n",
    "    if (text_size > blank_size) and (auth_size > blank_size):\n",
    "        size_check_success = True\n",
    "    else:\n",
    "        size_check_success = False\n",
    "    if ((text_mtime - rxiv_mtime) < timedelta(days=3)) and ((auth_mtime - rxiv_mtime) < timedelta(days=3)):\n",
    "        rxiv_check_success = True\n",
    "    else:\n",
    "        rxiv_check_success = False\n",
    "    if ((text_mtime - lit_mtime) < timedelta(days=3)) and ((auth_mtime - lit_mtime) < timedelta(days=3)):\n",
    "        lit_check_success = True\n",
    "    else:\n",
    "        lit_check_success = False\n",
    "    runcheck_dict = {\"size_check_success\":size_check_success,\n",
    "                     \"preprint_check_success\":rxiv_check_success,\n",
    "                     \"litcovid_check_success\":lit_check_success}\n",
    "    return(runcheck_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions:\n",
    "\n",
    "The original preprint matching script was divided up into 3 parts with save/load points each\n",
    "**update metadata**\n",
    "1. Load previous preprint and pmids\n",
    "2. Fetch all new preprint and pmids (save old and new preprint and pmids), include date for a check so it doesn't do this if within a day\n",
    "3. Fetch all data from new preprint and pmids\n",
    "4. Clean up metadata and save metadata\n",
    "\n",
    "**compare data**\n",
    "1. load metadata, use merges with preprint and pmid lists to determine old vs new\n",
    "2. compare New litcovid to old preprints (save data)\n",
    "3. compare New litcovid to new preprints (save data)\n",
    "4. compare New preprints to old Litcovid (save data)\n",
    "\n",
    "**clean up results**\n",
    "1. Check results of saved data for items meeting criteria\n",
    "2. generate results for appending data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check functionality of modularized script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "from datetime import datetime,timedelta\n",
    "import pathlib\n",
    "from src.update_functions import *\n",
    "from src.cleaning_functions import *\n",
    "\n",
    "#scriptpath = pathlib.Path(__file__).parent.absolute()\n",
    "scriptpath = ''\n",
    "RESULTSPATH = os.path.join(scriptpath,'results/')\n",
    "ARCHIVEPATH = os.path.join(RESULTSPATH,'archives/')\n",
    "TEMPPATH = os.path.join(RESULTSPATH,'temp/')\n",
    "\n",
    "update_meta(ARCHIVEPATH,TEMPPATH)\n",
    "\n",
    "#### Run time = 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### This code performs the actual bag of word comparisons\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "from datetime import datetime,timedelta\n",
    "import pathlib\n",
    "from src.comparison_functions import *\n",
    "\n",
    "#scriptpath = pathlib.Path(__file__).parent.absolute()\n",
    "#try:\n",
    "#    generalpath = pathlib.Path(__file__).parents[1].absolute()\n",
    "#except:\n",
    "#    generalpath = pathlib.Path(__file__).resolve().parents[1].absolute()\n",
    "\n",
    "script_path = ''\n",
    "general_path = os.path.abspath(os.path.join(os.getcwd(),\"../\"))\n",
    "\n",
    "\n",
    "RESULTSPATH = os.path.join(scriptpath,'results/')\n",
    "ARCHIVEPATH = os.path.join(RESULTSPATH,'archives/')\n",
    "TEMPPATH = os.path.join(RESULTSPATH,'temp/')\n",
    "TOPICPATH = os.path.join(generalpath,'topic_classifier/results/') \n",
    "TOPICFILE = read_csv(os.path.join(TOPICPATH,'topicCats.tsv'),delimiter='\\t',header=0,index_col=0,\n",
    "                     converters={\"topicCategory\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "TOPICFILE.fillna({i: [] for i in TOPICFILE.index})\n",
    "topicdf = TOPICFILE.explode('topicCategory').reset_index()\n",
    "topicdf.drop(columns=['index'],inplace=True)\n",
    "\n",
    "thresholds = {\"auth\":0.45,\n",
    "              \"text\":0.2,\n",
    "              \"sum_min\":0.75}\n",
    "\n",
    "clean_rxiv_text,clean_rxiv_auth,clean_lit_text,clean_lit_auth = load_new_meta(TEMPPATH)\n",
    "old_rxiv_text,old_rxiv_auth,old_lit_text,old_lit_auth =  load_previous_runs(ARCHIVEPATH)\n",
    "\n",
    "new_rxiv = check_b4_compare(clean_rxiv_text,old_rxiv_text)\n",
    "new_litcovid = check_b4_compare(clean_rxiv_text,old_rxiv_text)\n",
    "\n",
    "if new_rxiv==True and new_litcovid==True:\n",
    "    blank_temps(TEMPPATH)\n",
    "    ## run old preprints against new litcovid entries:\n",
    "    if len(clean_lit_text)>0:\n",
    "        run_comparison(clean_lit_text,old_rxiv_text,'text',thresholds,TEMPPATH)\n",
    "    if len(clean_lit_auth)>0:\n",
    "        run_comparison(old_rxiv_auth,clean_lit_auth,'auth', thresholds,TEMPPATH)\n",
    "\n",
    "    ## run new preprints against new litcovid entries\n",
    "    if len(clean_rxiv_text)>0:\n",
    "        run_comparison(clean_rxiv_text,clean_lit_text,'text',thresholds,TEMPPATH)\n",
    "\n",
    "    if len(clean_rxiv_auth)>0:\n",
    "        run_comparison(clean_rxiv_auth,clean_lit_auth,'auth', thresholds,TEMPPATH)\n",
    "\n",
    "\n",
    "elif new_rxiv==False and new_litcovid==True:\n",
    "    ## run old preprints against new litcovid entries\n",
    "    if len(clean_lit_text)>0:\n",
    "        run_comparison(old_rxiv_text,clean_lit_text,'text',thresholds,TEMPPATH)\n",
    "\n",
    "    if len(clean_lit_auth)>0:\n",
    "        run_comparison(old_rxiv_auth,clean_lit_auth,'auth', thresholds,TEMPPATH)\n",
    "\n",
    "\n",
    "elif new_rxiv==True and new_litcovid==False:\n",
    "    print(\"no point in comparing new preprints to old litcovid entries\")\n",
    "          \n",
    "else:\n",
    "    print(\"nothing new to compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_lit_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### clean_up_results.py\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime,timedelta\n",
    "import pathlib\n",
    "from src.comparison_functions import *\n",
    "from src.archive_functions import *\n",
    "\n",
    "\n",
    "#### Set paths\n",
    "#scriptpath = pathlib.Path(__file__).parent.absolute()\n",
    "scriptpath = ''\n",
    "RESULTSPATH = os.path.join(scriptpath,'results/')\n",
    "ARCHIVEPATH = os.path.join(RESULTSPATH,'archives/')\n",
    "TEMPPATH = os.path.join(RESULTSPATH,'temp/')\n",
    "OUTPUTPATH = os.path.join(RESULTSPATH,'update dumps/')\n",
    "REVIEWPATH = os.path.join(RESULTSPATH,'to review/')\n",
    "\n",
    "thresholds = {\"auth\":0.45,\n",
    "              \"text\":0.2,\n",
    "              \"sum_min\":0.75}\n",
    "\n",
    "## update the set after the run\n",
    "runcheck_dict = check_comparison_run(TEMPPATH)\n",
    "if False not in list(runcheck_dict.values()):\n",
    "    clean_rxiv_text,clean_rxiv_auth,clean_lit_text,clean_lit_auth = load_new_meta(TEMPPATH)\n",
    "    update_precompute(clean_lit_text,ARCHIVEPATH)\n",
    "    update_precompute(clean_lit_auth,ARCHIVEPATH)\n",
    "    update_precompute(clean_rxiv_text,ARCHIVEPATH)\n",
    "    update_precompute(clean_rxiv_auth,ARCHIVEPATH)\n",
    "    try:\n",
    "        new_text_matches = read_csv(os.path.join(TEMPPATH,'text_above_threshold.txt'),delimiter='\\t',header=0,index_col=False)\n",
    "        if 'Unnamed: 0' in new_text_matches.columns:\n",
    "            new_text_matches.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    except:\n",
    "        new_text_matches = pd.DataFrame(columns=['litcovid','preprint','j_sim'])\n",
    "    try:\n",
    "        new_auth_matches = read_csv(os.path.join(TEMPPATH,'auth_above_threshold.txt'),delimiter='\\t',header=0,index_col=False)\n",
    "        if 'Unnamed: 0' in new_auth_matches.columns:\n",
    "            new_auth_matches.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    except:\n",
    "        new_auth_matches = pd.DataFrame(columns=['litcovid','preprint','j_sim'])\n",
    "\n",
    "    if len(new_text_matches)<1 or len(new_auth_matches)<1:\n",
    "        matchupdates = False\n",
    "    else:\n",
    "        matchupdates = True\n",
    "        clean_matches,lowscores,manual_check = sort_matches(new_text_matches,new_auth_matches,thresholds)\n",
    "    corrections_added = generate_updates(clean_matches,OUTPUTPATH)\n",
    "    split_corrections_added = generate_split_updates(clean_matches,OUTPUTPATH)\n",
    "    manual_check_update = update_results(manual_check,ARCHIVEPATH,REVIEWPATH)\n",
    "    lowscores_update = update_results(lowscores,ARCHIVEPATH,REVIEWPATH)\n",
    "    clean_match_update = update_results(clean_matches,ARCHIVEPATH,REVIEWPATH)\n",
    "else:\n",
    "    print(runcheck_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished:update_data.py\n",
      "Finished:compare_data.py\n",
      "Finished:clean_up_results.py\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Sequentially run all scripts needed for preprint-matching\n",
    "import subprocess\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "#scriptpath = pathlib.Path(__file__).parent.absolute()\n",
    "scriptpath = ''\n",
    "update_script = os.path.join(scriptpath,'update_data.py')\n",
    "compare_script = os.path.join(scriptpath,'compare_data.py')\n",
    "cleanup_script = os.path.join(scriptpath,'clean_up_results.py')\n",
    "program_list = [update_script,compare_script,cleanup_script]\n",
    "\n",
    "for program in program_list:\n",
    "    subprocess.call(['python', program])\n",
    "    print(\"Finished:\" + program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of the main functions for trouble-shooting purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The comparison function parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_type = 'text'\n",
    "thresholds = {\"auth\":0.45,\n",
    "              \"text\":0.2,\n",
    "              \"sum_min\":0.75}\n",
    "\n",
    "#### Load annotations file\n",
    "TOPICFILE = read_csv(os.path.join(TOPICPATH,'topicCats.tsv'),delimiter='\\t',header=0,index_col=0,\n",
    "                     converters={\"topicCategory\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "TOPICFILE.fillna({i: [] for i in TOPICFILE.index})\n",
    "topicdf = TOPICFILE.explode('topicCategory').reset_index()\n",
    "topicdf.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "update_meta(ARCHIVEPATH,TEMPPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## load the metadata for comparison\n",
    "clean_rxiv_text,clean_rxiv_auth,clean_lit_text,clean_lit_auth = load_new_meta(TEMPPATH)\n",
    "old_rxiv_text,old_rxiv_auth,old_lit_text,old_lit_auth =  load_previous_runs(ARCHIVEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_temps(TEMPPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison Functions of binning by topicCategory is desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## run old preprints against new litcovid entries:\n",
    "if len(clean_lit_text)>0:\n",
    "    for eachtopic in topicdf['topicCategory'].unique().tolist():\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_text,old_rxiv_text,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'text',thresholds,TEMPPATH)\n",
    "    try:\n",
    "        eachtopic = None\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_text,clean_rxiv_text,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'text',thresholds,TEMPPATH)\n",
    "    except:\n",
    "        pass\n",
    "if len(clean_lit_auth)>0:\n",
    "    for eachtopic in topicdf['topicCategory'].unique().tolist():\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_auth,old_rxiv_auth,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'auth', thresholds,TEMPPATH)\n",
    "    try:\n",
    "        eachtopic = None\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_text,clean_rxiv_text,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'text',thresholds,TEMPPATH)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## run new preprints against new litcovid entries:\n",
    "if len(clean_lit_text)>0:\n",
    "    for eachtopic in topicdf['topicCategory'].unique().tolist():\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_text,old_rxiv_text,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'text',thresholds,TEMPPATH)\n",
    "    try:\n",
    "        eachtopic = None\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_text,clean_rxiv_text,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'text',thresholds,TEMPPATH)\n",
    "    except:\n",
    "        pass\n",
    "if len(clean_lit_auth)>0:\n",
    "    for eachtopic in topicdf['topicCategory'].unique().tolist():\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_auth,old_rxiv_auth,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'auth', thresholds,TEMPPATH)\n",
    "    try:\n",
    "        eachtopic = None\n",
    "        preprint_topicdf,litcovid_topicdf = generate_comparison_dfs(topicdf,clean_lit_text,clean_rxiv_text,eachtopic)\n",
    "        if len(preprint_topicdf)+len(litcovid_topicdf)>0:\n",
    "            run_comparison(preprint_topicdf,litcovid_topicdf,'text',thresholds,TEMPPATH)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison function without binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_rxiv==True and new_litcovid==True:\n",
    "    blank_temps(TEMPPATH)\n",
    "    ## run old preprints against new litcovid entries:\n",
    "    if len(clean_lit_text)>0:\n",
    "        run_comparison(clean_lit_text,old_rxiv_text,'text',thresholds,TEMPPATH)\n",
    "    if len(clean_lit_auth)>0:\n",
    "        run_comparison(old_rxiv_auth,clean_lit_auth,'auth', thresholds,TEMPPATH)\n",
    "\n",
    "    ## run new preprints against new litcovid entries\n",
    "    if len(clean_rxiv_text)>0:\n",
    "        run_comparison(clean_rxiv_text,clean_lit_text,'text',thresholds,TEMPPATH)\n",
    "\n",
    "    if len(clean_rxiv_auth)>0:\n",
    "        run_comparison(clean_rxiv_auth,clean_lit_auth,'auth', thresholds,TEMPPATH)\n",
    "\n",
    "\n",
    "elif new_rxiv==False and new_litcovid==True:\n",
    "    ## run old preprints against new litcovid entries\n",
    "    if len(clean_lit_text)>0:\n",
    "        run_comparison(old_rxiv_text,clean_lit_text,'text',thresholds,TEMPPATH)\n",
    "\n",
    "    if len(clean_lit_auth)>0:\n",
    "        run_comparison(old_rxiv_auth,clean_lit_auth,'auth', thresholds,TEMPPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the set after the run\n",
    "runcheck_dict = check_comparison_run(TEMPPATH)\n",
    "if False not in list(runcheck_dict.values()):\n",
    "    update_precompute(clean_lit_text,ARCHIVEPATH)\n",
    "    update_precompute(clean_lit_auth,ARCHIVEPATH)\n",
    "    update_precompute(clean_rxiv_text,ARCHIVEPATH)\n",
    "    update_precompute(clean_rxiv_auth,ARCHIVEPATH)\n",
    "    try:\n",
    "        new_text_matches = read_csv(os.path.join(TEMPPATH,'text_above_threshold.txt'),delimiter='\\t',header=0,index_col=False)\n",
    "        if 'Unnamed: 0' in new_text_matches.columns:\n",
    "            new_text_matches.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    except:\n",
    "        new_text_matches = pd.DataFrame(columns=['litcovid','preprint','j_sim'])\n",
    "    try:\n",
    "        new_auth_matches = read_csv(os.path.join(TEMPPATH,'auth_above_threshold.txt'),delimiter='\\t',header=0,index_col=False)\n",
    "        if 'Unnamed: 0' in new_auth_matches.columns:\n",
    "            new_auth_matches.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    except:\n",
    "        new_auth_matches = pd.DataFrame(columns=['litcovid','preprint','j_sim'])\n",
    "\n",
    "    if len(new_text_matches)<1 or len(new_auth_matches)<1:\n",
    "        matchupdates = False\n",
    "    else:\n",
    "        matchupdates = True\n",
    "        clean_matches,lowscores,manual_check = sort_matches(new_text_matches,new_auth_matches,thresholds)\n",
    "    corrections_added = generate_updates(clean_matches,OUTPUTPATH)\n",
    "    split_corrections_added = generate_split_updates(clean_matches,OUTPUTPATH)\n",
    "    manual_check_update = update_results(manual_check,ARCHIVEPATH,REVIEWPATH)\n",
    "    lowscores_update = update_results(lowscores,ARCHIVEPATH,REVIEWPATH)\n",
    "    clean_match_update = update_results(clean_matches,ARCHIVEPATH,REVIEWPATH)\n",
    "else:\n",
    "    print(runcheck_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
